tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokenizer(data['prompt'].tolist(), padding=True, truncation=True, return_tensors="pt")

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(y_train.unique()))

training_args = TrainingArguments(
      output_dir='./results',         
      num_train_epochs=3,              
      per_device_train_batch_size=16,  
      per_device_eval_batch_size=64,   
      warmup_steps=500,                
      weight_decay=0.01,               
      logging_dir='./logs',            
      logging_steps=10,  
      optim="adamw_torch", 
) 

trainer = Trainer(
      model=model,                         
      args=training_args,                   
      train_dataset=train_dataset,          
      eval_dataset=test_dataset            
)  